{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrPbe9DSvzwrds+nbsD8gS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendikardian/exercise_machine_learning/blob/main/imageClassification_transferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKQ_q3WUrIF7",
        "outputId": "e9af5ed5-5b46-4bc6-8b49-1c725ef4bbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 11:12:21--  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip [following]\n",
            "--2023-12-25 11:12:22--  https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60684125 (58M) [application/zip]\n",
            "Saving to: ‘/tmp/Chessman-image-dataset.zip’\n",
            "\n",
            "/tmp/Chessman-image 100%[===================>]  57.87M   130MB/s    in 0.4s    \n",
            "\n",
            "2023-12-25 11:12:24 (130 MB/s) - ‘/tmp/Chessman-image-dataset.zip’ saved [60684125/60684125]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip \\\n",
        "  -O /tmp/Chessman-image-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "local_zip = '/tmp/Chessman-image-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "train_dir = os.path.join('/tmp/Chessman-image-dataset/Chess')\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode = 'nearest',\n",
        "    validation_split=0.1) # set validation split"
      ],
      "metadata": {
        "id": "wJ9Sd1qfrOrA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # same directory as training data\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqKG_5VDrTq4",
        "outputId": "63573aad-4780-46ce-f27c-c47db867306f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 499 images belonging to 6 classes.\n",
            "Found 52 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "model = tf.keras.models.Sequential([\n",
        "    ResNet152V2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(150, 150, 3))),\n",
        "    # tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.layers[0].trainable = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKPTnfSerUjB",
        "outputId": "dfc09b0b-43be-4348-be24-a037781b1e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234545216/234545216 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "yb-pT_3Brljl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              epochs=50,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sedV9XCPrqbV",
        "outputId": "bf272f49-74d7-4ee4-e6ff-6fa642ba1bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 - 175s - loss: 9.9416 - accuracy: 0.4068 - val_loss: 1.7664 - val_accuracy: 0.6731 - 175s/epoch - 3s/step\n",
            "Epoch 2/50\n",
            "63/63 - 169s - loss: 1.9622 - accuracy: 0.6794 - val_loss: 1.1598 - val_accuracy: 0.7115 - 169s/epoch - 3s/step\n",
            "Epoch 3/50\n",
            "63/63 - 159s - loss: 0.9519 - accuracy: 0.7495 - val_loss: 1.2202 - val_accuracy: 0.6923 - 159s/epoch - 3s/step\n",
            "Epoch 4/50\n",
            "63/63 - 163s - loss: 0.6430 - accuracy: 0.8357 - val_loss: 1.0408 - val_accuracy: 0.7308 - 163s/epoch - 3s/step\n",
            "Epoch 5/50\n",
            "63/63 - 159s - loss: 0.6424 - accuracy: 0.8076 - val_loss: 1.0996 - val_accuracy: 0.6538 - 159s/epoch - 3s/step\n",
            "Epoch 6/50\n",
            "63/63 - 167s - loss: 0.7141 - accuracy: 0.8297 - val_loss: 0.9415 - val_accuracy: 0.7115 - 167s/epoch - 3s/step\n",
            "Epoch 7/50\n",
            "63/63 - 161s - loss: 0.4734 - accuracy: 0.8677 - val_loss: 1.1163 - val_accuracy: 0.7308 - 161s/epoch - 3s/step\n",
            "Epoch 8/50\n",
            "63/63 - 163s - loss: 0.4089 - accuracy: 0.8798 - val_loss: 1.0244 - val_accuracy: 0.7885 - 163s/epoch - 3s/step\n",
            "Epoch 9/50\n",
            "63/63 - 161s - loss: 0.4014 - accuracy: 0.8938 - val_loss: 1.1588 - val_accuracy: 0.7115 - 161s/epoch - 3s/step\n",
            "Epoch 10/50\n",
            "63/63 - 163s - loss: 0.6684 - accuracy: 0.8417 - val_loss: 2.2486 - val_accuracy: 0.6731 - 163s/epoch - 3s/step\n",
            "Epoch 11/50\n",
            "63/63 - 168s - loss: 0.6412 - accuracy: 0.8758 - val_loss: 0.7793 - val_accuracy: 0.7885 - 168s/epoch - 3s/step\n",
            "Epoch 12/50\n",
            "63/63 - 164s - loss: 0.6005 - accuracy: 0.8778 - val_loss: 1.1824 - val_accuracy: 0.7500 - 164s/epoch - 3s/step\n",
            "Epoch 13/50\n",
            "63/63 - 162s - loss: 0.3328 - accuracy: 0.8858 - val_loss: 1.1546 - val_accuracy: 0.6923 - 162s/epoch - 3s/step\n",
            "Epoch 14/50\n",
            "63/63 - 162s - loss: 0.3330 - accuracy: 0.9198 - val_loss: 0.8612 - val_accuracy: 0.8269 - 162s/epoch - 3s/step\n",
            "Epoch 15/50\n",
            "63/63 - 153s - loss: 0.4262 - accuracy: 0.9158 - val_loss: 1.5870 - val_accuracy: 0.7308 - 153s/epoch - 2s/step\n",
            "Epoch 16/50\n",
            "63/63 - 163s - loss: 0.3437 - accuracy: 0.9299 - val_loss: 0.7608 - val_accuracy: 0.7885 - 163s/epoch - 3s/step\n",
            "Epoch 17/50\n",
            "63/63 - 162s - loss: 0.2324 - accuracy: 0.9359 - val_loss: 1.0727 - val_accuracy: 0.7885 - 162s/epoch - 3s/step\n",
            "Epoch 18/50\n",
            "63/63 - 155s - loss: 0.3970 - accuracy: 0.9218 - val_loss: 1.5739 - val_accuracy: 0.7692 - 155s/epoch - 2s/step\n",
            "Epoch 19/50\n",
            "63/63 - 162s - loss: 0.4137 - accuracy: 0.9078 - val_loss: 1.6374 - val_accuracy: 0.7308 - 162s/epoch - 3s/step\n",
            "Epoch 20/50\n",
            "63/63 - 163s - loss: 0.2555 - accuracy: 0.9439 - val_loss: 1.0446 - val_accuracy: 0.7885 - 163s/epoch - 3s/step\n",
            "Epoch 21/50\n",
            "63/63 - 163s - loss: 0.1508 - accuracy: 0.9579 - val_loss: 0.5206 - val_accuracy: 0.8654 - 163s/epoch - 3s/step\n",
            "Epoch 22/50\n",
            "63/63 - 165s - loss: 0.1832 - accuracy: 0.9419 - val_loss: 1.9141 - val_accuracy: 0.6731 - 165s/epoch - 3s/step\n",
            "Epoch 23/50\n"
          ]
        }
      ]
    }
  ]
}